{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5isHANSFo2-g"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import accuracy_score\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
        "\n",
        "\n",
        "class TextModel(nn.Module):\n",
        "    def __init__(self,model_name=None,num_labels=1):\n",
        "        super(TextModel,self).__init__()\n",
        "        config = AutoConfig.from_pretrained(model_name)\n",
        "        self.model = AutoModel.from_pretrained(model_name) # 768\n",
        "        self.drop_out = nn.Dropout(0.1)\n",
        "        self.dropout1 = nn.Dropout(0.1)\n",
        "        self.dropout2 = nn.Dropout(0.2)\n",
        "        self.dropout3 = nn.Dropout(0.3)\n",
        "        self.dropout4 = nn.Dropout(0.4)\n",
        "        self.dropout5 = nn.Dropout(0.5)\n",
        "        self.output = nn.Linear(config.hidden_size,num_labels)\n",
        "\n",
        "        if 'deberta-v2-xxlarge' in model_name:\n",
        "            self.model.embeddings.requires_grad_(False)\n",
        "            self.model.encoder.layer[:24].requires_grad_(False) # 24/48\n",
        "        if 'deberta-v2-xlarge' in model_name:\n",
        "            self.model.embeddings.requires_grad_(False)\n",
        "            self.model.encoder.layer[:12].requires_grad_(False) # 12/24\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, labels=None):\n",
        "        if 'gpt' in self.model.name_or_path:\n",
        "            emb = self.model(input_ids)[0]\n",
        "        else:\n",
        "            emb = self.model(input_ids,attention_mask)[0]\n",
        "\n",
        "        preds1 = self.output(self.dropout1(emb))\n",
        "        preds2 = self.output(self.dropout2(emb))\n",
        "        preds3 = self.output(self.dropout3(emb))\n",
        "        preds4 = self.output(self.dropout4(emb))\n",
        "        preds5 = self.output(self.dropout5(emb))\n",
        "        preds = (preds1 + preds2 + preds3 + preds4 + preds5) / 5\n",
        "\n",
        "        logits = torch.softmax(preds, dim=-1)\n",
        "        if labels is not None:\n",
        "            loss = self.get_loss(preds,labels,attention_mask)\n",
        "            return loss,logits\n",
        "        else:\n",
        "            return logits\n",
        "\n",
        "    def get_loss(self, outputs, targets, attention_mask):\n",
        "        loss_fct = nn.CrossEntropyLoss()\n",
        "\n",
        "        active_loss = attention_mask.reshape(-1) == 1\n",
        "        active_logits = outputs.reshape(-1, outputs.shape[-1])\n",
        "        true_labels = targets.reshape(-1)\n",
        "        idxs = np.where(active_loss.cpu().numpy() == 1)[0]\n",
        "        active_logits = active_logits[idxs]\n",
        "        true_labels = true_labels[idxs].to(torch.long)\n",
        "\n",
        "        loss = loss_fct(active_logits, true_labels)\n",
        "\n",
        "        return loss"
      ]
    }
  ]
}