{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Seminar 1: Fun with Word Embeddings**\n",
        "Today we are gonna play with word embeddings: train our own little embeddings, load one from gensim model zoo and use it to visualize text corpora.\n",
        "\n",
        "This whole thing is gonna happen on top of embedding dataset.\n",
        "\n",
        "Requirements: pip install --upgrade nltk gensim bokeh , but only if you're running locally."
      ],
      "metadata": {
        "id": "ISq7UocFG3po"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# download the data:\n",
        "!wget https://www.dropbox.com/s/obaitrix9jyu84r/quora.txt?dl=1 -O ./quora.txt # questions on quora (people ask)\n",
        "# alternative download link: https://yadi.sk/i/BPQrUu1NaTduEw"
      ],
      "metadata": {
        "id": "pbb3spNI9QXq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('quora.txt', encoding=\"utf-8\") as f:\n",
        "    data = list(f)\n",
        "\n",
        "data[10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "BXd2OGwg9QaC",
        "outputId": "0248665a-7ad0-416c-cd87-5c27d05ca444"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Which brand should go with the GTX 960 graphic card, MSI, Zotac or ASUS?\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(data) # number of sentences of our data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iXMNWzFv9Qcg",
        "outputId": "557c241b-ccb2-4b31-9e41-86f05d9295d8"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "537272"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tokenization**: a typical first step for an NLP task is to split raw data into words. The text we're working with is in raw format: with all the punctuation and smiles attached to some words, so a simple str.split won't do.\n",
        "\n",
        "Let's use nltk - a library that handles many NLP tasks like tokenization, stemming or part-of-speech tagging.\n",
        "\n"
      ],
      "metadata": {
        "id": "2_QiWLk4ITpr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from nltk.tokenize import  WordPunctTokenizer\n",
        "\n",
        "tokenizer = WordPunctTokenizer()\n",
        "\n",
        "print(tokenizer.tokenize(data[11]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ygiXpHwB9Qjr",
        "outputId": "f8ccae1a-6550-4296-a9f3-751332fe05ca"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['What', 'is', 'the', 'ZIP', 'code', 'of', 'India', '?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TASK: lowercase everything and extract tokens with tokenizer.\n",
        "# data_tok should be a list of lists of tokens for each line in data.\n",
        "\n",
        "data_tok = [tokenizer.tokenize(i.lower()) for i in data]"
      ],
      "metadata": {
        "id": "a9PV2mgrHtXp"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_tok[11]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zNrvJkrdHtaA",
        "outputId": "ca90ee9c-aeb8-482c-ac6b-7b98f744d5b7"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['what', 'is', 'the', 'zip', 'code', 'of', 'india', '?']"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "assert all(isinstance(row, (list, tuple)) for row in data_tok), \"please convert each line into a list of tokens (strings)\"\n",
        "assert all(all(isinstance(tok, str) for tok in row) for row in data_tok), \"please convert each line into a list of tokens (strings)\"\n",
        "is_latin = lambda tok: all('a' <= x.lower() <= 'z' for x in tok)\n",
        "assert all(map(lambda l: not is_latin(l) or l.islower(), map(' '.join, data_tok))), \"please make sure to lowercase the data\""
      ],
      "metadata": {
        "id": "tOWh1phGHtcU"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print([' '.join(row) for row in data_tok[:1]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qZ6Uox1AHtfF",
        "outputId": "8fb71fdc-e1d7-4fb8-c742-b4a5344b5ece"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"can i get back with my ex even though she is pregnant with another guy ' s baby ?\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Word vectors**: as the saying goes, there's more than one way to train word embeddings. There's Word2Vec and GloVe with different objective functions. Then there's fasttext that uses character-level models to train word embeddings.\n",
        "\n",
        "The choice is huge, so let's start someplace small: gensim is another nlp library that features many vector-based models incuding word2vec."
      ],
      "metadata": {
        "id": "p-of6VV0KU_J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "model = Word2Vec(\n",
        "    data_tok,\n",
        "    vector_size=32, # embedding vector size\n",
        "    window=5, # define context as a 5-word window around the target word\n",
        "    min_count=5 # consider words that occured at least 5 times (отбрасывает самые встречаемые слова >=5 (здесь))\n",
        ").wv"
      ],
      "metadata": {
        "id": "ZueGWVOK9QmE"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now you can get word vectors !\n",
        "\n",
        "model.get_vector('everything')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ESti5XiC9QoZ",
        "outputId": "fe33681b-2d4a-4add-8400-c23c1afe0ad8"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-1.5262333 ,  2.1145294 ,  0.81044066,  3.0935519 ,  1.2803497 ,\n",
              "        1.9093763 , -1.45781   , -5.0202804 , -0.02520465,  0.9337871 ,\n",
              "        1.4796574 ,  0.30828473,  1.1404164 ,  0.33315268,  2.5724516 ,\n",
              "       -2.7664094 ,  0.8886677 , -1.3391675 , -0.2556421 , -0.67231786,\n",
              "       -1.3501204 , -0.29044598, -0.43982   , -1.4151559 , -0.18784814,\n",
              "       -0.94874924,  0.08696299,  1.6951393 ,  1.0397223 ,  0.2070252 ,\n",
              "       -0.6495235 ,  0.25012478], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# or query similar words directly. Go play with it!\n",
        "\n",
        "model.most_similar('bread')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FDyS4Ynw9QsH",
        "outputId": "c4ea8ce7-5a1d-430c-ac23-a2d05ad165da"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('rice', 0.9542890787124634),\n",
              " ('sauce', 0.9335342645645142),\n",
              " ('butter', 0.9222909212112427),\n",
              " ('cheese', 0.9192749857902527),\n",
              " ('beans', 0.9166175127029419),\n",
              " ('fruit', 0.909808874130249),\n",
              " ('pasta', 0.9089231491088867),\n",
              " ('potato', 0.899134635925293),\n",
              " ('wine', 0.8983882665634155),\n",
              " ('vodka', 0.8936192989349365)]"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader as api\n",
        "wv = api.load('word2vec-google-news-300') # 300-dim vectors for each token"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-1HTGftm9Qvw",
        "outputId": "adad7e79-f96f-411f-9f6d-962e43326276"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vec_everything = wv['everything']\n",
        "#vec_everything"
      ],
      "metadata": {
        "id": "utaL7zb69Q0L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wv.most_similar('bread')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oWO-3VCR9Q22",
        "outputId": "c5f5852d-7b28-4f23-bddb-0b5a560c99d9"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('butter', 0.6417260766029358),\n",
              " ('rye_sourdough', 0.6290417313575745),\n",
              " ('breads', 0.6243128180503845),\n",
              " ('loaf', 0.6184971332550049),\n",
              " ('flour', 0.615212619304657),\n",
              " ('baladi_bread', 0.6061378121376038),\n",
              " ('loaves', 0.6045446991920471),\n",
              " ('raisin_bread', 0.5843341946601868),\n",
              " ('stale_bread', 0.5802395343780518),\n",
              " ('wheaten_flour', 0.5785929560661316)]"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "as we can see: **the more size of corpora, the better**"
      ],
      "metadata": {
        "id": "m5W3Mf05TVv_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Using pre-trained model**\n",
        "Took it a while, huh? Now imagine training life-sized (100~300D) word embeddings on gigabytes of text: wikipedia articles or twitter posts.\n",
        "\n",
        "Thankfully, nowadays you can get a pre-trained word embedding model in 2 lines of code (no sms required, promise)."
      ],
      "metadata": {
        "id": "BCYuxmheb0i3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "model = api.load('glove-twitter-100')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YHyRJUWR9Q5A",
        "outputId": "3095e7cf-4166-4a9c-f3e9-048e56f1431e"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 387.1/387.1MB downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#api.info() # all the corporas of gensim"
      ],
      "metadata": {
        "id": "lIqP0oO3eJHR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.most_similar(positive=[\"queen\", 'man'], negative=[\"woman\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tk2kB-kKTgnM",
        "outputId": "0b328e66-ec7f-4352-affa-634777a0dc89"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('king', 0.6708807945251465),\n",
              " ('aka', 0.6319987177848816),\n",
              " ('fan', 0.6134569048881531),\n",
              " ('rock', 0.6048598289489746),\n",
              " ('sorry', 0.5993564128875732),\n",
              " ('song', 0.5918704271316528),\n",
              " ('jessie', 0.5864764451980591),\n",
              " ('boy', 0.5861239433288574),\n",
              " ('punk', 0.5848831534385681),\n",
              " (\"'s\", 0.5833798050880432)]"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Visualizing phrases**\n",
        "Word embeddings can also be used to represent short phrases. The simplest way is to take **an average of vectors** for all tokens in the phrase with some weights.\n",
        "\n",
        "This trick is useful to identify what data are you working with: find if there are any outliers, clusters or other artefacts.\n",
        "\n",
        "Let's try this new hammer on our data!"
      ],
      "metadata": {
        "id": "nN3X2HjZcLk2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_phrase_embedding(phrase):\n",
        "    \"\"\"\n",
        "    Convert phrase to a vector by aggregating it's word embeddings. See description above.\n",
        "    \"\"\"\n",
        "    # 1. lowercase phrase\n",
        "    # 2. tokenize phrase\n",
        "    # 3. average word vectors for all words in tokenized phrase\n",
        "    # skip words that are not in model's vocabulary\n",
        "    # if all words are missing from vocabulary, return zeros\n",
        "\n",
        "    vector = np.zeros([model.vector_size], dtype='float32')\n",
        "    phrase = tokenizer.tokenize(phrase.lower())\n",
        "    vocab = model.key_to_index.keys()\n",
        "    for word in phrase:\n",
        "        if word in vocab:\n",
        "            vector += np.array(model.get_vector(word))\n",
        "\n",
        "    return vector"
      ],
      "metadata": {
        "id": "MChsf0iUTgpp"
      },
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vector = get_phrase_embedding(\"I'm very sure. This never happened to me before...\")\n",
        "vector[::10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FC8pwpHpnfWT",
        "outputId": "cdb6cb19-8970-4c2b-b12e-0710bb510dd5"
      },
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([  3.8168845 ,  -0.3069805 ,   1.1199516 ,  -1.2026184 ,\n",
              "       -12.334427  ,  -1.994626  ,   0.61000896,   2.1587763 ,\n",
              "        16.44223   ,   1.038716  ], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 134
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# let's only consider ~5k phrases for a first run.\n",
        "chosen_phrases = data[::len(data) // 1000]\n",
        "\n",
        "# compute vectors for chosen phrases\n",
        "phrase_vectors = [get_phrase_embedding(phrase) for phrase in chosen_phrases]"
      ],
      "metadata": {
        "id": "Yxx8OegiTgyl"
      },
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = get_phrase_embedding('What is the best programming language?')"
      ],
      "metadata": {
        "id": "D5coIA-CqfMM"
      },
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#cosines = [vec @ query / np.linalg.norm(vec) / np.linalg.norm(query) for vec in phrase_vectors]\n",
        "cosines = [np.nan_to_num(vec @ query / np.linalg.norm(vec) / np.linalg.norm(query)) for vec in phrase_vectors]\n",
        "# the second one works better (there isn't any trash, like 'AALKFLKSAJFJL')"
      ],
      "metadata": {
        "id": "leEgx0PIqfOS"
      },
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in np.argsort(cosines)[-5:]:\n",
        "    print(chosen_phrases[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JThAaJohqfSA",
        "outputId": "23460c97-2d3e-481e-ae89-769418ee326b"
      },
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What is the best modern programming language?\n",
            "\n",
            "What is the best programming language in 2016?\n",
            "\n",
            "Which is the best programming language?\n",
            "\n",
            "What is the best IoT programming language?\n",
            "\n",
            "What is the best programming language?\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JFpXTga9qfUh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OM5O-pE8qfW2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JGTLGqdHqfZY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rjO8GPF0qfc-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oI72GNmgTg9q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x-nYMPCqThAA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hcyXcbDeThCs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rbwwBisETgOi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SkWmlgJbTgRG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gNL-y6ok9Q7c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wALwlDwG9Q94"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}