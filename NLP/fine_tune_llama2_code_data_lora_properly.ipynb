{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "48d431504f6c4c04a2ab1b2435c2f8bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a115f6217504495bb0a1779016816a69",
              "IPY_MODEL_f66cea1120c949faaa574227f7244d6b",
              "IPY_MODEL_5fe1efeb2b294b2eb7affa3149fe1169"
            ],
            "layout": "IPY_MODEL_3e82519c6ef044ad8d2b3cf536105826"
          }
        },
        "a115f6217504495bb0a1779016816a69": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d6bc4b958f264b8d949abe301f8488ab",
            "placeholder": "​",
            "style": "IPY_MODEL_24126fd7947c49f89f14ec56dceb6626",
            "value": "Map: 100%"
          }
        },
        "f66cea1120c949faaa574227f7244d6b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ca8a4da4c9734bc8a1e439d9655b7e91",
            "max": 1366,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5de69a6810954ed0838d48f0dacb8aba",
            "value": 1366
          }
        },
        "5fe1efeb2b294b2eb7affa3149fe1169": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7302787810454024ade76ddea113d172",
            "placeholder": "​",
            "style": "IPY_MODEL_8c19e3a0ac974eb8a19eadf61642bb71",
            "value": " 1366/1366 [00:00&lt;00:00, 7937.00 examples/s]"
          }
        },
        "3e82519c6ef044ad8d2b3cf536105826": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d6bc4b958f264b8d949abe301f8488ab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "24126fd7947c49f89f14ec56dceb6626": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ca8a4da4c9734bc8a1e439d9655b7e91": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5de69a6810954ed0838d48f0dacb8aba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7302787810454024ade76ddea113d172": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8c19e3a0ac974eb8a19eadf61642bb71": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "87f845ffe61b466a8762c69ef4dbe629": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7e012c3d343345e0b61e160b22087d3e",
              "IPY_MODEL_61d24c58b9394f35b6cf1809f00a7fe0",
              "IPY_MODEL_34465102f4994745bbfc626f774e10f6"
            ],
            "layout": "IPY_MODEL_fd70c8b8b16b4a879f0e6eacf3b5e88e"
          }
        },
        "7e012c3d343345e0b61e160b22087d3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_69a9896f9afe4fe594dd87535b3ab4f2",
            "placeholder": "​",
            "style": "IPY_MODEL_20b70c2489444769945a1b99dd5a71dc",
            "value": "Map: 100%"
          }
        },
        "61d24c58b9394f35b6cf1809f00a7fe0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a145cdd970a4411ea2594e885cb53c4a",
            "max": 1366,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_50280bd360dc43239e6924a9da87c479",
            "value": 1366
          }
        },
        "34465102f4994745bbfc626f774e10f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f12e03cd540b4dbda8b372ad72aa5e9a",
            "placeholder": "​",
            "style": "IPY_MODEL_651dd99f59004ee28a67c8bc9919ffe1",
            "value": " 1366/1366 [00:01&lt;00:00, 621.27 examples/s]"
          }
        },
        "fd70c8b8b16b4a879f0e6eacf3b5e88e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "69a9896f9afe4fe594dd87535b3ab4f2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "20b70c2489444769945a1b99dd5a71dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a145cdd970a4411ea2594e885cb53c4a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "50280bd360dc43239e6924a9da87c479": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f12e03cd540b4dbda8b372ad72aa5e9a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "651dd99f59004ee28a67c8bc9919ffe1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b39bcd5a392e49c9a0b7d35cf1a0c541": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0d590478d1b942479465c6e44a2627fc",
              "IPY_MODEL_3bc0330e78db4f12818be7cd0d394cf2",
              "IPY_MODEL_7e2102a634ab465d8fc80f2d9f2fae38"
            ],
            "layout": "IPY_MODEL_9b2adb8fedbb42d58b1d020e4c30e2fe"
          }
        },
        "0d590478d1b942479465c6e44a2627fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d293a9ff4e4546838f543aa44ac23b26",
            "placeholder": "​",
            "style": "IPY_MODEL_4967471a299b42d0a8a2caf86b89feb6",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "3bc0330e78db4f12818be7cd0d394cf2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_05696fa846f04e8786078273f84690b7",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_10f2b6dccd15451989f353fb1e88b01f",
            "value": 2
          }
        },
        "7e2102a634ab465d8fc80f2d9f2fae38": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e044e57a9f7e49a49b753bd6c9261d73",
            "placeholder": "​",
            "style": "IPY_MODEL_572c3f1a897c47899a96aa2eb8717227",
            "value": " 2/2 [01:15&lt;00:00, 34.47s/it]"
          }
        },
        "9b2adb8fedbb42d58b1d020e4c30e2fe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d293a9ff4e4546838f543aa44ac23b26": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4967471a299b42d0a8a2caf86b89feb6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "05696fa846f04e8786078273f84690b7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "10f2b6dccd15451989f353fb1e88b01f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e044e57a9f7e49a49b753bd6c9261d73": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "572c3f1a897c47899a96aa2eb8717227": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "GkrIWNKnJxgT"
      },
      "outputs": [],
      "source": [
        "!pip install datasets accelerate bitsandbytes trl peft -U -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import transformers\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, BitsAndBytesConfig, DataCollatorForLanguageModeling\n",
        "import bitsandbytes as bnb\n",
        "from trl import SFTTrainer\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "import datasets\n",
        "from accelerate import Accelerator\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "OPQ0je4pJx4o"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = datasets.load_dataset('flytech/llama-python-codes-30k', split='train')"
      ],
      "metadata": {
        "id": "tOY0hcRwJx7c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03c95104-28cd-47de-f9d1-dc083868a804"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g2USEfIgJx-Q",
        "outputId": "e7088d62-e7f0-43b6-aa45-54baf9a7f2a9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['instruction', 'input', 'output', 'text'],\n",
              "    num_rows: 27332\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# FOR TEST THE TRAINING (get rid of it if you want to train a model on a full data)\n",
        "shuffled_dataset = dataset.shuffle(seed=42)\n",
        "subset_size = int(0.05 * len(shuffled_dataset)) # 5% от исходного датасета\n",
        "subset = shuffled_dataset.select(range(subset_size))\n",
        "subset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xwfxe2QtcxcM",
        "outputId": "dd78c53c-5451-4e53-d3ad-f126ffa36ff3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['instruction', 'input', 'output', 'text'],\n",
              "    num_rows: 1366\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## count max_length of our tokenized_data"
      ],
      "metadata": {
        "id": "LpTfnB-y5_TL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'NousResearch/Llama-2-7b-chat-hf'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True,\n",
        "                                          add_eos_token=True, add_bos_token=True)"
      ],
      "metadata": {
        "id": "CCjL6wiv5_m1"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(example):\n",
        "  prompt = f\"Question: {example['instruction']}\\n Answer: {example['output']}\"\n",
        "  result = tokenizer(prompt)\n",
        "  result['labels'] = result['input_ids'].copy()\n",
        "  return result"
      ],
      "metadata": {
        "id": "te_LeIpi5_pV"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "subset_tokenizer_for_getting_max_length = subset.map(tokenize)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0,
          "referenced_widgets": [
            "87f845ffe61b466a8762c69ef4dbe629",
            "7e012c3d343345e0b61e160b22087d3e",
            "61d24c58b9394f35b6cf1809f00a7fe0",
            "34465102f4994745bbfc626f774e10f6",
            "fd70c8b8b16b4a879f0e6eacf3b5e88e",
            "69a9896f9afe4fe594dd87535b3ab4f2",
            "20b70c2489444769945a1b99dd5a71dc",
            "a145cdd970a4411ea2594e885cb53c4a",
            "50280bd360dc43239e6924a9da87c479",
            "f12e03cd540b4dbda8b372ad72aa5e9a",
            "651dd99f59004ee28a67c8bc9919ffe1"
          ]
        },
        "id": "y3BpN7Mi6rqi",
        "outputId": "a8b96fbb-0ae3-49bf-cbcb-191e951c8bb9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/1366 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "87f845ffe61b466a8762c69ef4dbe629"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(subset_tokenizer_for_getting_max_length['input_ids'][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "-laQWCL86rtV",
        "outputId": "8d44c623-4b7f-4061-e706-608a4825481f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"<s> Question: [INST]Execute code: from flask import Flask\\napp = Flask(__name__)\\n\\n@app.route('/')\\ndef hello_world():\\n    return 'Hello, World!'\\n\\nif __name__ == '__main__':\\n    app.run()[/INST]\\n Answer: ```python\\nfrom flask import Flask\\napp = Flask(__name__)\\n\\n@app.route('/')\\ndef hello_world():\\n    return 'Hello, World!'\\n\\nif __name__ == '__main__':\\n    app.run()\\n# Code executed.\\n```</s>\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(subset_tokenizer_for_getting_max_length['input_ids'][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LlDYGRng8voM",
        "outputId": "f1e40f7e-754a-44d5-be78-7ace3b4d5a37"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "135"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "maxi = 0\n",
        "for i in range(len(subset_tokenizer_for_getting_max_length)):\n",
        "  maxi = max(maxi, len(subset_tokenizer_for_getting_max_length['input_ids'][i]))\n",
        "maxi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pD1Jp6lc8ZZc",
        "outputId": "b339969c-a0b7-4e31-9f55-6133e80e4b83"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "497"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_data_lengths(tokenized_dataset):\n",
        "    lengths = [len(x['input_ids']) for x in tokenized_dataset]\n",
        "    print(len(lengths))\n",
        "\n",
        "    # Plotting the histogram\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.hist(lengths, bins=20, alpha=0.7, color='blue')\n",
        "    plt.xlabel('Length of input_ids')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.title('Distribution of Lengths of input_ids')\n",
        "    plt.show()\n",
        "\n",
        "plot_data_lengths(subset_tokenizer_for_getting_max_length)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "6Xv--vGQ9IsJ",
        "outputId": "5a913bcc-a602-452b-906d-df1807ebe66a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1366\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cN_MkLnx8Zcm"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## continue"
      ],
      "metadata": {
        "id": "-BOzRbZ45_0T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = 497 # we found it out above\n",
        "\n",
        "model_name = 'NousResearch/Llama-2-7b-chat-hf'\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    model_name, padding_side='left',\n",
        "    add_eos_token=True, add_bos_token=True\n",
        ")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "def formatting_func(example):\n",
        "    prompt = f\"Question: {example['instruction']}\\n Answer: {example['output']}\"\n",
        "    result = tokenizer(\n",
        "        prompt,\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
        "    return result"
      ],
      "metadata": {
        "id": "3xIIkXWN9-mD"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "subset = subset.map(formatting_func)"
      ],
      "metadata": {
        "id": "jQ-r9n-i_nX3"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_dataset = subset.train_test_split(test_size=0.2)\n",
        "train_dataset, eval_dataset = new_dataset['train'], new_dataset['test']\n",
        "train_dataset, eval_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TZVR2p5y_nas",
        "outputId": "e4ff3a96-7a06-48b3-e069-90df6ba73cf0"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Dataset({\n",
              "     features: ['instruction', 'input', 'output', 'text', 'input_ids', 'attention_mask', 'labels'],\n",
              "     num_rows: 1092\n",
              " }),\n",
              " Dataset({\n",
              "     features: ['instruction', 'input', 'output', 'text', 'input_ids', 'attention_mask', 'labels'],\n",
              "     num_rows: 274\n",
              " }))"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(train_dataset['input_ids'][4])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "T3GadgMz_ngn",
        "outputId": "cbca2e69-10ef-4cf9-b393-c363baf5230e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'</s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s><s> Question: [INST]Design an algorithm in Python to check if an integer is a power of two Number: 16[/INST]\\n Answer: ```python\\ndef isPowerOfTwo(n):\\n  if(n == 0): \\n    return False\\n  while(n != 1): \\n    if(n % 2 != 0): \\n      return False\\n    n = n // 2\\n    \\n  return True\\n  \\n# The following call will return True\\nisPowerOfTwo(16)\\n```</s>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Let's check how base model deal with it first (before fine-tuning)"
      ],
      "metadata": {
        "id": "7rg_hE3iAr4d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_quant_type='nf4',\n",
        "    bnb_4bit_use_double_quant=True\n",
        ")"
      ],
      "metadata": {
        "id": "tbVv_30DBmaE"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'NousResearch/Llama-2-7b-chat-hf'\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map='auto',\n",
        "    low_cpu_mem_usage=True, offload_state_dict=True\n",
        ")"
      ],
      "metadata": {
        "id": "a2MZyRJSBmaF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "b39bcd5a392e49c9a0b7d35cf1a0c541",
            "0d590478d1b942479465c6e44a2627fc",
            "3bc0330e78db4f12818be7cd0d394cf2",
            "7e2102a634ab465d8fc80f2d9f2fae38",
            "9b2adb8fedbb42d58b1d020e4c30e2fe",
            "d293a9ff4e4546838f543aa44ac23b26",
            "4967471a299b42d0a8a2caf86b89feb6",
            "05696fa846f04e8786078273f84690b7",
            "10f2b6dccd15451989f353fb1e88b01f",
            "e044e57a9f7e49a49b753bd6c9261d73",
            "572c3f1a897c47899a96aa2eb8717227"
          ]
        },
        "outputId": "abbfe9f3-33f2-428e-bc8a-6e8528951900"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b39bcd5a392e49c9a0b7d35cf1a0c541"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Question: \" + train_dataset[0]['instruction'])\n",
        "print(\"Answer: \" + train_dataset[0]['output'] + \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-BHCjuRFAjMK",
        "outputId": "5a96a578-ecfc-487a-d857-c5058740ba8a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: [INST]Create an expression parser in Python to evaluate basic mathematical expressions[/INST]\n",
            "Answer: ```python\n",
            "# Assume input is a valid mathematical expression with no syntax errors \n",
            "\n",
            "def expression_parser(expression): \n",
            "    tokens = expression.split() \n",
            "    stack = [] \n",
            "  \n",
            "    for token in tokens: \n",
            "        if token == '+' or token == '*' or token == '/': \n",
            "            a = stack.pop() \n",
            "            b = stack.pop() \n",
            "            if token == '+': \n",
            "                c = b + a \n",
            "            elif token == '*': \n",
            "                c = b * a \n",
            "            else: \n",
            "                c = b / a \n",
            "            stack.append(c) \n",
            "        else: \n",
            "            stack.append(int(token)) \n",
            "    return stack.pop()\n",
            "```\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eval_prompt = f\"Question: {train_dataset[0]['instruction']}\\n Answer: \""
      ],
      "metadata": {
        "id": "LU9LuJrEAjPR"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_tokenizer = AutoTokenizer.from_pretrained(\n",
        "    model_name,\n",
        "    add_bos_token=True\n",
        ")\n",
        "\n",
        "model_input = eval_tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    print(eval_tokenizer.decode(model.generate(**model_input, max_new_tokens=256)[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4cjcSZK-AjiX",
        "outputId": "eb76d6de-8a7f-4fe9-b1d0-ea98345f1080"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: [INST]Create an expression parser in Python to evaluate basic mathematical expressions[/INST]\n",
            " Answer: \n",
            "\n",
            "Creating an expression parser in Python to evaluate basic mathematical expressions can be done using a combination of regular expressions and a dictionary of mathematical operators. Here is an example implementation:\n",
            "\n",
            "First, define a dictionary of mathematical operators and their associated functions:\n",
            "```\n",
            "operators = {\n",
            "    '+': lambda x, y: x + y,\n",
            "    '-': lambda x, y: x - y,\n",
            "    '*': lambda x, y: x * y,\n",
            "    '/': lambda x, y: x / y,\n",
            "    '//': lambda x, y: x // y,\n",
            "    '**': lambda x, y: x ** y,\n",
            "}\n",
            "```\n",
            "Next, define a regular expression to match mathematical expressions:\n",
            "```\n",
            "expression_regex = r'[+\\-*\\/]?([0-9]+([\\+\\-*\\/]?[0-9])?)'\n",
            "```\n",
            "This regular expression matches any sequence of digits, optional operators, and optional more digits. The `?` flag makes the operator optional, and the `([0-9]+` part matches one or more digits.\n",
            "\n",
            "Now, you can define a function to parse and evaluate expressions using the regular expression\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Getting LoRA and fine-tune"
      ],
      "metadata": {
        "id": "tDr_D4LnDiw4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.gradient_checkpointing_enable()\n",
        "#model.enable_input_require_grads()\n",
        "model = prepare_model_for_kbit_training(model)"
      ],
      "metadata": {
        "id": "RblNXq6TAjlG"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_trainable_parameters(model):\n",
        "    \"\"\"\n",
        "    Prints the number of trainable parameters in the model.\n",
        "    \"\"\"\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_param += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "    print(\n",
        "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
        "    )"
      ],
      "metadata": {
        "id": "RjzQC6gvDiR4"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "def get_num_layers(model):\n",
        "    numbers = set()\n",
        "    for name, _ in model.named_parameters():\n",
        "        for number in re.findall(r'\\d+', name):\n",
        "            numbers.add(int(number))\n",
        "    return max(numbers)\n",
        "\n",
        "def get_last_layer_linears(model):\n",
        "    names = []\n",
        "\n",
        "    num_layers = get_num_layers(model)\n",
        "    for name, module in model.named_modules():\n",
        "        if str(num_layers) in name and not \"encoder\" in name:\n",
        "            if isinstance(module, torch.nn.Linear):\n",
        "                names.append(name)\n",
        "    return names"
      ],
      "metadata": {
        "id": "pJQPr1L95qtF"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_num_layers(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xOVf1q5d6ZWf",
        "outputId": "0a21ada6-17da-45d5-e22d-8693b6904e54"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "31"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "get_last_layer_linears(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oSzlu065ZIZp",
        "outputId": "3abbc7bd-fe9a-4c5d-c103-55d48ac6c6d4"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['model.layers.31.self_attn.q_proj',\n",
              " 'model.layers.31.self_attn.k_proj',\n",
              " 'model.layers.31.self_attn.v_proj',\n",
              " 'model.layers.31.self_attn.o_proj',\n",
              " 'model.layers.31.mlp.gate_proj',\n",
              " 'model.layers.31.mlp.up_proj',\n",
              " 'model.layers.31.mlp.down_proj']"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=16,\n",
        "    target_modules=get_last_layer_linears(model), # only_last_layer\n",
        "    #[   \"q_proj\", # all_layers\n",
        "    #    \"k_proj\",\n",
        "    #    \"v_proj\",\n",
        "    #    \"o_proj\",\n",
        "    #    \"gate_proj\",\n",
        "    #    \"up_proj\",\n",
        "    #    \"down_proj\",\n",
        "    #    \"lm_head\",\n",
        "    #],\n",
        "    bias=\"none\",\n",
        "    lora_dropout=0.05,\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, config)\n",
        "print_trainable_parameters(model)\n",
        "\n",
        "accelerator = Accelerator()\n",
        "model = accelerator.prepare_model(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m40LxVXzDiUX",
        "outputId": "243a5901-1ff1-4b0f-f620-e704f7ec8023"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 624640 || all params: 3501037568 || trainable%: 0.017841568045693132\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q wandb -U\n",
        "\n",
        "import wandb, os\n",
        "wandb.login()\n",
        "\n",
        "wandb.init(project=\"llama2-code-fine-tune\", name=\"1\")"
      ],
      "metadata": {
        "id": "XrE_86SjDiXR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "outputId": "893bf26b-3e90-4357-cd81-d5961dfb2237"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/7.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/7.1 MB\u001b[0m \u001b[31m105.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m128.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m77.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/207.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.8/301.8 kB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdanilka200300\u001b[0m (\u001b[33mdanilka200300-misis\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.17.6"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20240812_185242-pktt2l9a</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/danilka200300-misis/llama2-code-fine-tune/runs/pktt2l9a' target=\"_blank\">1</a></strong> to <a href='https://wandb.ai/danilka200300-misis/llama2-code-fine-tune' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/danilka200300-misis/llama2-code-fine-tune' target=\"_blank\">https://wandb.ai/danilka200300-misis/llama2-code-fine-tune</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/danilka200300-misis/llama2-code-fine-tune/runs/pktt2l9a' target=\"_blank\">https://wandb.ai/danilka200300-misis/llama2-code-fine-tune/runs/pktt2l9a</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/danilka200300-misis/llama2-code-fine-tune/runs/pktt2l9a?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x78d60de9e710>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.device_count() > 1:\n",
        "    model.is_parallelizable = True\n",
        "    model.model_parallel = True"
      ],
      "metadata": {
        "id": "PNjJLGIMGEBX"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "collate_fn = DataCollatorForLanguageModeling(tokenizer, mlm=False)"
      ],
      "metadata": {
        "id": "ZiCi6_RBHUZ1"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "project = \"llama2-code-fine-tune\"\n",
        "base_model_name = \"llama2-7b\"\n",
        "run_name = base_model_name + \"-\" + project\n",
        "output_dir = \"./\" + run_name\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "        output_dir=output_dir,\n",
        "        warmup_steps=5,\n",
        "        per_device_train_batch_size=8,\n",
        "        gradient_checkpointing=True,\n",
        "        gradient_accumulation_steps=1,\n",
        "        max_steps=1000,\n",
        "        learning_rate=2.5e-5,\n",
        "        logging_steps=25,\n",
        "        bf16=True,\n",
        "        optim=\"paged_adamw_8bit\",\n",
        "        logging_dir=\"./logs\",\n",
        "        save_strategy=\"steps\",\n",
        "        save_steps=25,\n",
        "        eval_strategy=\"steps\",\n",
        "        eval_steps=25,\n",
        "        do_eval=True,\n",
        "        report_to=\"wandb\",\n",
        "    )"
      ],
      "metadata": {
        "id": "wjDTrWKOHpnG"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = 497 # we found it out above\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    peft_config=config,\n",
        "    args=training_args,\n",
        "    data_collator=collate_fn,\n",
        "    max_seq_length=max_length\n",
        ")\n",
        "\n",
        "model.config.use_cache = False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i_H53wqAGED-",
        "outputId": "2f3452b1-0121-4b8e-b8c2-adfb8cc8ce86"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.\n",
            "\n",
            "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
            "  warnings.warn(message, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:280: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:408: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n",
            "  warnings.warn(\n",
            "max_steps is given, it will override any value given in num_train_epochs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "id": "EJH1frUTGEGf",
        "outputId": "e977e5ba-ad32-4c68-9622-a696a55f0d1b"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [   3/1000 05:30 < 91:31:51, 0.00 it/s, Epoch 0.06/30]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-3435b262f1ae>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    449\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trl_activate_neftune\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 451\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m         \u001b[0;31m# After training we make sure to retrieve back the original forward pass method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1930\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1931\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1932\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   1933\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1934\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2267\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccumulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2268\u001b[0;31m                     \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2270\u001b[0m                 if (\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3322\u001b[0m                 \u001b[0mscaled_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3323\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3324\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3326\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient_accumulation_steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2157\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlomo_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2158\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2159\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2161\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_trigger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    523\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m             )\n\u001b[0;32m--> 525\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    526\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    268\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 744\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    745\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "a5JBTvdmGEKU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WO1k0OE-It7Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## THROW AWAY (BUT USEFUL INFO CAN BE FOUND HERE TOO)"
      ],
      "metadata": {
        "id": "UKDe3OLmJAAk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def formatting_func(example):\n",
        "  return {'text': f\"Question: {example['instruction']}\\n Answer: {example['output']}\"}"
      ],
      "metadata": {
        "id": "_aVAEqR5JyAR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#dataset = dataset.map(formatting_func)\n",
        "subset = subset.map(formatting_func)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "48d431504f6c4c04a2ab1b2435c2f8bd",
            "a115f6217504495bb0a1779016816a69",
            "f66cea1120c949faaa574227f7244d6b",
            "5fe1efeb2b294b2eb7affa3149fe1169",
            "3e82519c6ef044ad8d2b3cf536105826",
            "d6bc4b958f264b8d949abe301f8488ab",
            "24126fd7947c49f89f14ec56dceb6626",
            "ca8a4da4c9734bc8a1e439d9655b7e91",
            "5de69a6810954ed0838d48f0dacb8aba",
            "7302787810454024ade76ddea113d172",
            "8c19e3a0ac974eb8a19eadf61642bb71"
          ]
        },
        "id": "UTyv91PpJyC2",
        "outputId": "d5a92972-4e96-43b7-d02a-efb2dd3c982e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/1366 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "48d431504f6c4c04a2ab1b2435c2f8bd"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#dataset['text'][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "5GjJXMAWJyFH",
        "outputId": "aeea57ac-3f84-4cc3-9904-79c365ff2bd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"<s>[INST]Help me set up my daily to-do list![/INST] Setting up your daily to-do list... ```python\\ntasks = []\\nwhile True:\\n    task = input('Enter a task or type 'done' to finish: ')\\n    if task == 'done': break\\n    tasks.append(task)\\nprint(f'Your to-do list for today: {tasks}')\\n```</s>\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "subset['text'][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "C8lo9N2nxQ8d",
        "outputId": "6281c1d3-a4ba-424b-b1f4-b3398064e330"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Question: [INST]Execute code: from flask import Flask\\napp = Flask(__name__)\\n\\n@app.route('/')\\ndef hello_world():\\n    return 'Hello, World!'\\n\\nif __name__ == '__main__':\\n    app.run()[/INST]\\n Answer: ```python\\nfrom flask import Flask\\napp = Flask(__name__)\\n\\n@app.route('/')\\ndef hello_world():\\n    return 'Hello, World!'\\n\\nif __name__ == '__main__':\\n    app.run()\\n# Code executed.\\n```\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_dataset = subset.train_test_split(test_size=0.2) #dataset.train_test_split(test_size=0.2)\n",
        "new_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G_rETAjtY_3z",
        "outputId": "78c0cb19-bc77-4318-a0b7-0a27a7ee1c4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['instruction', 'input', 'output', 'text'],\n",
              "        num_rows: 1092\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['instruction', 'input', 'output', 'text'],\n",
              "        num_rows: 274\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset, eval_dataset = new_dataset['train'], new_dataset['test']\n",
        "train_dataset, eval_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nbnKV3o1cKgQ",
        "outputId": "6b21401e-2721-422a-9b55-a53d74c47ac8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Dataset({\n",
              "     features: ['instruction', 'input', 'output', 'text'],\n",
              "     num_rows: 1092\n",
              " }),\n",
              " Dataset({\n",
              "     features: ['instruction', 'input', 'output', 'text'],\n",
              "     num_rows: 274\n",
              " }))"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_quant_type='nf4',\n",
        "    bnb_4bit_use_double_quant=True\n",
        ")"
      ],
      "metadata": {
        "id": "uRamb9KXJyKI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'NousResearch/Llama-2-7b-chat-hf'\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map={\"\": 0}\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "tokenizer.padding_side = 'left'"
      ],
      "metadata": {
        "id": "9F3jI2_NMGgH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.gradient_checkpointing_enable()\n",
        "model = prepare_model_for_kbit_training(model)"
      ],
      "metadata": {
        "id": "TRU432345f30"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "peft_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.1,\n",
        "    bias='none',\n",
        "    task_type='CAUSAL_LM'\n",
        ")"
      ],
      "metadata": {
        "id": "AYyrUsl4MGis"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install wandb -q"
      ],
      "metadata": {
        "id": "NAeJJZVZXAOw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wandb login"
      ],
      "metadata": {
        "id": "v3TTbUwFXBDQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "wandb.init(project=\"llama2-code-fine-tune\", name=\"1\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "dqYSwAzmWzPv",
        "outputId": "db29659d-5504-46a0-8900-c5a081b93f13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdanilka200300\u001b[0m (\u001b[33mdanilka200300-misis\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.17.6"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20240812_065835-3wbggrfr</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/danilka200300-misis/llama2-code-fine-tune/runs/3wbggrfr' target=\"_blank\">1</a></strong> to <a href='https://wandb.ai/danilka200300-misis/llama2-code-fine-tune' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/danilka200300-misis/llama2-code-fine-tune' target=\"_blank\">https://wandb.ai/danilka200300-misis/llama2-code-fine-tune</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/danilka200300-misis/llama2-code-fine-tune/runs/3wbggrfr' target=\"_blank\">https://wandb.ai/danilka200300-misis/llama2-code-fine-tune/runs/3wbggrfr</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/danilka200300-misis/llama2-code-fine-tune/runs/3wbggrfr?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x78c89e230310>"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir='./output',\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=8,\n",
        "    gradient_checkpointing=True,\n",
        "    gradient_accumulation_steps=1,\n",
        "    learning_rate=3e-4,\n",
        "    optim='paged_adamw_32bit',\n",
        "    weight_decay=0.001,\n",
        "    warmup_ratio = 0.03,\n",
        "    group_by_length = True,\n",
        "    lr_scheduler_type = \"cosine\",\n",
        "    max_steps = -1,\n",
        "    max_grad_norm=0.3,\n",
        "    log_level=\"info\",\n",
        "    disable_tqdm=False,\n",
        "\n",
        "    report_to=\"wandb\",\n",
        "    eval_steps=1,\n",
        "    logging_steps=1,\n",
        "    do_eval=True,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    save_strategy=\"steps\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    save_total_limit=3\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uzpYB0o_MGlu",
        "outputId": "5d974ce7-1541-4d09-c4e0-5dc812aecdce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "PyTorch: setting up devices\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1btEpkyCzuV6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### NEED TO DEAL WITH THIS PROBLEM WITH 'Answer: ' not found and model unability to calculate loss\n",
        "train_dataset['text'][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "xCphQR6Zw8PR",
        "outputId": "9380ba72-b752-4140-fab9-b30d35e94c7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Question: [INST]How to do sentiment analysis on news articles?[/INST]\\n Answer: ```python\\n# Doing sentiment analysis on news articles.\\nfrom textblob import TextBlob\\n# Scrape news articles.\\n# Use TextBlob to analyze the sentiment of the article.\\n```'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(train_dataset)):\n",
        "  print(train_dataset['text'][i])\n",
        "  break"
      ],
      "metadata": {
        "id": "HCQr1zRgzRml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MSkPcDp3zrwp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "template='Answer: '\n",
        "collate_fn = DataCollatorForCompletionOnlyLM(response_template=template, tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "qDAEFMAiQdB7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    peft_config=peft_config,\n",
        "    args=training_args,\n",
        "    max_seq_length=2048,\n",
        "    data_collator=collate_fn,\n",
        "    packing=False\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 602
        },
        "id": "2_UNCeMsMGoy",
        "outputId": "c982127c-c259-48f4-c113-97d3b06a36bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.\n",
            "\n",
            "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
            "  warnings.warn(message, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "PyTorch: setting up devices\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "PyTorch: setting up devices\n",
            "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:280: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'list' object has no attribute 'column_names'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-99-79b316007b19>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m trainer = SFTTrainer(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0meval_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     99\u001b[0m                     \u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"\\n\\n\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcustom_message\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics, peft_config, dataset_text_field, packing, formatting_func, max_seq_length, infinite, num_of_sequences, chars_per_token, dataset_num_proc, dataset_batch_size, neftune_noise_alpha, model_init_kwargs, dataset_kwargs, eval_packing)\u001b[0m\n\u001b[1;32m    371\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtrain_dataset\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 373\u001b[0;31m                 train_dataset = self._prepare_dataset(\n\u001b[0m\u001b[1;32m    374\u001b[0m                     \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m                     \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py\u001b[0m in \u001b[0;36m_prepare_dataset\u001b[0;34m(self, dataset, tokenizer, packing, dataset_text_field, max_seq_length, formatting_func, num_of_sequences, chars_per_token, remove_unused_columns, append_concat_token, add_special_tokens, skip_prepare_dataset)\u001b[0m\n\u001b[1;32m    517\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mpacking\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m             return self._prepare_non_packed_dataloader(\n\u001b[0m\u001b[1;32m    520\u001b[0m                 \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m                 \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py\u001b[0m in \u001b[0;36m_prepare_non_packed_dataloader\u001b[0;34m(self, tokenizer, dataset, dataset_text_field, max_seq_length, formatting_func, add_special_tokens, remove_unused_columns)\u001b[0m\n\u001b[1;32m    577\u001b[0m         \u001b[0msignature_columns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"labels\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"attention_mask\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m         \u001b[0mextra_columns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumn_names\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msignature_columns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mremove_unused_columns\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextra_columns\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'column_names'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "g1pKOnKeMGri",
        "outputId": "60ee554d-cce4-43b3-b870-aafe068c4916"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running training *****\n",
            "  Num examples = 1,092\n",
            "  Num Epochs = 1\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 137\n",
            "  Number of trainable parameters = 8,388,608\n",
            "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
            "/usr/local/lib/python3.10/dist-packages/trl/trainer/utils.py:187: UserWarning: Could not find response key `Answer: ` in the following instance: <s> Question: [INST]Implement a Binary Search Tree (BST) in Python. The BST should have  methods for insertion, search and traversal[/INST]\n",
            " Answer: ```python\n",
            "class Node:\n",
            " def __init__(self, data):\n",
            " self.left = None\n",
            " self.right = None\n",
            " self.data = data\n",
            "\n",
            "    \n",
            "class BST:\n",
            " def __init__(self):\n",
            " self.root = None\n",
            " \n",
            " def insert(self, data):\n",
            " if self.root == None:\n",
            " self.root = Node(data)\n",
            " else:\n",
            " self._insert(data, self.root)\n",
            " \n",
            " def _insert(self, data, curr_node):\n",
            " if data < curr_node.data:\n",
            " if curr_node.left == None:\n",
            " curr_node.left = Node(data)\n",
            " else:\n",
            " self._insert(data, curr_node.left)\n",
            " \n",
            " elif data > curr_node.data:\n",
            " if curr_node.right == None:\n",
            " curr_node.right = Node(data)\n",
            " else:\n",
            " self._insert(data, curr_node.right)\n",
            " else:\n",
            " print(\"Value already present in tree\")\n",
            " \n",
            " def search(self, data):\n",
            " if self.root != None:\n",
            " return self._search(data, self.root)\n",
            " else:\n",
            " return False\n",
            " \n",
            " def _search(self, data, curr_node):\n",
            " if data == curr_node.data:\n",
            " return True\n",
            " elif data < curr_node.data and curr_node.left != None:\n",
            " return self._search(data, curr_node.left)\n",
            " elif data > curr_node.data and curr_node.right != None:\n",
            " return self._search(data, curr_node.right)\n",
            " \n",
            " def traverse(self):\n",
            " if self.root != None:\n",
            " self._traverse(self.root)\n",
            " \n",
            " def _traverse(self, curr_node):\n",
            " if curr_node.left != None:\n",
            " self._traverse(curr_node.left)\n",
            " \n",
            " print(curr_node.data)\n",
            " \n",
            " if curr_node.right != None:\n",
            " self._traverse(curr_node.right)\n",
            "``` This instance will be ignored in loss calculation. Note, if this happens often, consider increasing the `max_seq_length`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/trl/trainer/utils.py:187: UserWarning: Could not find response key `Answer: ` in the following instance: <s> Question: [INST]Trying to import `'requests'`. \n",
            "\n",
            "\n",
            "Has it installed via `pip3` install requests? But still, have this error.\n",
            "\n",
            "\n",
            "\n",
            "```\n",
            "C:\\Users\\Vikentiy>pip3 list\n",
            "Package    Version\n",
            "---------- ----------\n",
            "certifi    2018.11.29\n",
            "chardet    3.0.4\n",
            "Django     2.1.7\n",
            "idna       2.8\n",
            "pip        19.0.2\n",
            "pytz       2018.9\n",
            "requests   2.21.0\n",
            "setuptools 40.6.2\n",
            "simplejson 3.16.0\n",
            "urllib3    1.24.1\n",
            "virtualenv 16.4.0\n",
            "\n",
            "C:\\Users\\Vikentiy>python --version\n",
            "Python 3.7.2\n",
            "\n",
            "```\n",
            "\n",
            "**Error Traceback:**\n",
            "\n",
            "\n",
            "\n",
            "```\n",
            "C:\\Users\\Vikentiy\\untitled2\\venv\\Scripts\\python.exe C:/Users/Vikentiy/untitled2/requeststests.py \n",
            "Traceback (most recent call last): \n",
            "File \"C:/Users/Vikentiy/untitled2/requeststests.py\", line 1, in <module> import requests`\n",
            "\n",
            "```\n",
            "[/INST]\n",
            " Answer: **Solution:**\n",
            "\n",
            "\n",
            "\n",
            "It seems like you are trying to import requests in a Python script but encountering an error. Here are a few steps you can try:\n",
            "\n",
            "1. Make sure you have installed requests for the correct version of Python. You seem to be using Python 3.7.2, so it should be installed via pip3.\n",
            "2. Try running your script in the same environment where you installed requests using `python script.py`. If you are using a virtual environment, activate it first.\n",
            "3. If you are still encountering the error, try uninstalling and reinstalling requests using `pip3 uninstall requests` and `pip3 install requests`.\n",
            "\n",
            "Hopefully, one of these steps will resolve your issue. Good luck!<unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk> This instance will be ignored in loss calculation. Note, if this happens often, consider increasing the `max_seq_length`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/trl/trainer/utils.py:187: UserWarning: Could not find response key `Answer: ` in the following instance: <s> Question: [INST]I want to connect to a web-server (a page) via a simple URL which already contains any parameters I want to sent, like : www.web-site.com/action.php/userid/42/secondpara/23/ and then get the page content which is generated by the site (won't be morde than a simple OK/NOK). How can I manage to do this? I failed to find any example-code or documentation which seems to fit my problem.\n",
            "\n",
            "\n",
            "Thx for helping.\n",
            "\n",
            "[/INST]\n",
            " Answer: To connect to a web server via a URL with parameters and retrieve the page content, you can use a combination of the HTTP request and response objects in Python. Here's some example code:\n",
            "\n",
            "```python\n",
            "import requests\n",
            "\n",
            "url = \"https://www.web-site.com/action.php/userid/42/secondpara/23/\"\n",
            "response = requests.get(url)\n",
            "\n",
            "if response.status\\_code == 200:\n",
            " print(\"OK\")\n",
            "else:\n",
            " print(\"NOK\")\n",
            "\n",
            "print(response.content)\n",
            "```\n",
            "\n",
            "The above code first imports the `requests` library, which is a popular library for making HTTP requests in Python. It then sets the `url` variable to the desired URL with parameters included. The `requests.get()` method is called with the `url` parameter, which sends an HTTP GET request to the server and returns a `Response` object.\n",
            "\n",
            "The `Response` object's `status\\_code` attribute is then checked to see if the request was successful (200 means \"Success\"), and the `content` attribute is printed to display the page content returned by the server.\n",
            "\n",
            "Note that you may need to add additional parameters to the `requests.get()` method, such as `headers` or `params`, depending on how the server is set up to handle requests. You can refer to the `requests` documentation for more information: https://docs.python-requests.org/en/latest/<unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk> This instance will be ignored in loss calculation. Note, if this happens often, consider increasing the `max_seq_length`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/trl/trainer/utils.py:187: UserWarning: Could not find response key `Answer: ` in the following instance: <s> Question: [INST]Eight handwritten poems by Hans Christian Andersen are to be found in Caroline Amalie?s library.\n",
            "\n",
            "Please remove spaces between words.[/INST]\n",
            " Answer: In order to remove spaces between words, I would follow these steps:\n",
            "\n",
            "Step 1:\n",
            "Obtain the digital copies of the eight handwritten poems by Hans Christian Andersen. I would need access to high-quality scans or photos of the original documents.\n",
            "\n",
            "Step 2:\n",
            "Transcribe the poems: If the poems are not already transcribed into digital text, I would use Optical Character Recognition (OCR) software to transcribe them. OCR software can identify and extract text from images, such as scanned documents or photos. Alternatively, a human can transcribe the text by typing out each line of the poem carefully.\n",
            "\n",
            "Step 3:\n",
            "Remove spaces: With the digital text version of the poems now available, I will then proceed to remove spaces between words of each poem. I would use a simple text processing script to locate and remove any space characters in the text. The script would iterate through each line of the poem, replacing space characters with an empty string.\n",
            "\n",
            "Here's a Python code snippet that illustrates this step:\n",
            "\n",
            "```python\n",
            "def remove_spaces(text):\n",
            "    return text.replace(\" \", \"\")\n",
            "\n",
            "poem_without_spaces = remove_spaces(poem)\n",
            "```\n",
            "\n",
            "Step 4:\n",
            "Save the modified poems to a file or other storage medium, maintaining the original formatting (line breaks, punctuation, etc.) without the spaces between words. These modified poems are now ready to be used as per the requested task.\n",
            "\n",
            "Though this would be the way to remove spaces between words digitally, it is important to remember that this request would be altering the original works of Hans Christian Andersen, which is not ideal from the perspective of literary integrity and respect for the author's intention.<unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk> This instance will be ignored in loss calculation. Note, if this happens often, consider increasing the `max_seq_length`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/trl/trainer/utils.py:187: UserWarning: Could not find response key `Answer: ` in the following instance: <s> Question: [INST]On my pdi transformations, a file is created and then a table on my database\\_1 is updated with the information on that file.\n",
            "That work perfectly fine.\n",
            "\n",
            "\n",
            "What i need now is to upload the file to some place in my web server. But I want to upload it **automaticaly** each time I finish the transformation described above.\n",
            "\n",
            "\n",
            "There is any job that could send a file to my web server?! or any other viable automated ways to do it?\n",
            "\n",
            "\n",
            "thank you.\n",
            "\n",
            "[/INST]\n",
            " Answer: Yes, there are a few different ways you could automate uploading the file to your web server. Here are a few options:\n",
            "\n",
            "1. Use an FTP client with scheduled tasks: You could use an FTP client like FileZilla or WinSCP to upload the file to your web server, and then use the scheduled tasks feature to automate the uploads. This would require setting up the FTP connection and configuring the scheduled tasks, but once it's set up, it should run without any additional input from you.\n",
            "\n",
            "2. Use a script with cron jobs: You could write a script in a programming language like Python or Bash that handles the file transfer, and then use cron jobs to run the script at specific times. This would require some programming knowledge, but it would give you more control over the file transfer process.\n",
            "\n",
            "3. Use a cloud-based file transfer service: There are several cloud-based file transfer services, like Dropbox or Google Drive, that can automatically sync files between your local machine and a web server. You could set up a folder on your local machine that the transformation program saves the file to, and then configure the file transfer service to upload the file to the web server whenever it appears in that folder.\n",
            "\n",
            "Ultimately, the best solution will depend on your specific needs and preferences.<unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk> This instance will be ignored in loss calculation. Note, if this happens often, consider increasing the `max_seq_length`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/trl/trainer/utils.py:187: UserWarning: Could not find response key `Answer: ` in the following instance: <s> Question: [INST]In this task, you are given inputs k, i, and A, where k and i are integers and A is a list. You need to find every ith element of A starting from the kth element. The process stops when the position of the next ith element exceeds the length of the list.\n",
            "\n",
            "2, 3, ['Z', 'v', '9199', 'W', 'N', 'v', '1997', 'C', 'J', '7437', '1269', 'T', 'k', 'Q'][/INST]\n",
            " Answer: Let me find the elements for you. \n",
            "\n",
            "k = 2\n",
            "i = 3\n",
            "A = ['Z', 'v', '9199', 'W', 'N', 'v', '1997', 'C', 'J', '7437', '1269', 'T', 'k', 'Q']\n",
            "\n",
            "First, we start at the kth element, which would have an index of (k - 1) because lists in Python are 0-indexed. In this case, the index is (2 - 1) = 1. Then, we count every ith element in the list starting from the kth element.\n",
            "\n",
            "The elements are:\n",
            "A[1] = 'v'\n",
            "A[1 + 3] = 'W'\n",
            "A[1 + 3 * 2] = '1997'\n",
            "A[1 + 3 * 3] = 'T'\n",
            "\n",
            "The next index would be (1 + 3 * 4) = 13, but this exceeds the length of the list, so we stop. \n",
            "\n",
            "The result is: ['v', 'W', '1997', 'T']<unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk> This instance will be ignored in loss calculation. Note, if this happens often, consider increasing the `max_seq_length`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/trl/trainer/utils.py:187: UserWarning: Could not find response key `Answer: ` in the following instance: <s> Question: [INST]Fix the code: from transformers import MarianMTModel, MarianTokenizer\n",
            "src_text = ['Hello, world!']\n",
            "model_name = 'Helsinki-NLP/opus-mt-en-de'\n",
            "model = MarianMTModel.from_pretrained(model_name)\n",
            "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
            "translated = model.generate(**tokenizer(src_text, return_tensors='pt', padding=True))[/INST]\n",
            " Answer: ```python\n",
            "from transformers import MarianMTModel, MarianTokenizer\n",
            "import torch\n",
            "def post_process_translation(translation_batch):\n",
            "    return [t.replace('<pad>', '').strip() for t in translation_batch]\n",
            "\n",
            "src_texts = ['Hello, world!', 'How are you?']\n",
            "model_name = 'Helsinki-NLP/opus-mt-en-de'\n",
            "model = MarianMTModel.from_pretrained(model_name)\n",
            "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
            "inputs = tokenizer(src_texts, return_tensors='pt', padding=True, truncation=True)\n",
            "translations = model.generate(**inputs)\n",
            "translated_texts = [tokenizer.decode(t, skip_special_tokens=True) for t in translations]\n",
            "final_translations = post_process_translation(translated_texts)\n",
            "\n",
            "# 1. Enabled batch translation.\n",
            "# 2. Added post-processing function to clean up the translations.\n",
            "# Executing code.\n",
            "```<unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk> This instance will be ignored in loss calculation. Note, if this happens often, consider increasing the `max_seq_length`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/trl/trainer/utils.py:187: UserWarning: Could not find response key `Answer: ` in the following instance: <s> Question: [INST]What's a fast way to verify whether all elements of an enumerable satisfy a certain condition? I guess logically it would be like:\n",
            "\n",
            "\n",
            "\n",
            "```\n",
            "elements = [e1, e2, e3, ...]\n",
            "return (condition on e1) && (condition on e2) && (condition on e3) && ...\n",
            "\n",
            "```\n",
            "\n",
            "For example, if I had an array of integers, and I wanted to answer the question \"Are all integers odd?\"\n",
            "\n",
            "\n",
            "I can always iterate over each value, check whether it's true, and then return false when one of them returns false, but is there a better way to do it?\n",
            "\n",
            "[/INST]\n",
            " Answer: Yes, you can use the \"all\" function in Python to quickly verify whether all elements of an enumerable satisfy a condition. \n",
            "\n",
            "For example, to check if all integers in a list are odd, you can use the following code:\n",
            "\n",
            "```python\n",
            "numbers = [1, 3, 5, 7, 9]\n",
            "all\\_odd = all(num % 2 != 0 for num in numbers)\n",
            "if all\\_odd:\n",
            " print(\"All numbers are odd\")\n",
            "else:\n",
            " print(\"Not all numbers are odd\")\n",
            "```\n",
            "\n",
            "The \"all\" function checks whether all elements in a given iterable (e.g. list, tuple, etc.) evaluate to true based on the condition provided. In this case, we check if each number in the list is odd (i.e. not divisible by 2). It returns True if all elements satisfy the condition, and False otherwise.\n",
            "\n",
            "By using this function, we avoid having to iterate over each element manually and can perform the check in a single line of code.<unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk> This instance will be ignored in loss calculation. Note, if this happens often, consider increasing the `max_seq_length`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/trl/trainer/utils.py:187: UserWarning: Could not find response key `Answer: ` in the following instance: <s> Question: [INST]What would be the best way to convert a given array to a LinkedList using Python? Array: [22, 7, 34, 15][/INST]\n",
            " Answer: ```python\n",
            "# Definition for singly-linked list.\n",
            "class ListNode:\n",
            "    def __init__(self, val=0, next=None):\n",
            "        self.val = val\n",
            "        self.next = next\n",
            "\n",
            "# arrayToList() function \n",
            "def arrayToList(arr): \n",
            "\n",
            "    # Create a reference to root \n",
            "    root = None\n",
            "\n",
            "    # Create a reference to prev node \n",
            "    prev = None\n",
            "\n",
            "    # Iterate through the given array \n",
            "    for x in arr: \n",
            "\n",
            "        # Allocate a new node in heap \n",
            "        curr = ListNode(x) \n",
            "\n",
            "        # Linked the prev node \n",
            "        # to the current node. \n",
            "        if prev != None: \n",
            "            prev.next = curr \n",
            "\n",
            "        # If root is None, \n",
            "        # assign it as curr node. \n",
            "        if root == None: \n",
            "            root = curr \n",
            "\n",
            "        # Store the current node \n",
            "        # as prev node for next \n",
            "        # iteration \n",
            "        prev = curr \n",
            "\n",
            "    # Return the root node of \n",
            "    # linked list. \n",
            "    return root \n",
            "\n",
            "# Sample array \n",
            "arr = [22, 7, 34, 15] \n",
            "\n",
            "# Root node reference \n",
            "root = arrayToList(arr) \n",
            "\n",
            "# Print the converted Linked List \n",
            "list_print(root)\n",
            "``` This instance will be ignored in loss calculation. Note, if this happens often, consider increasing the `max_seq_length`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/trl/trainer/utils.py:187: UserWarning: Could not find response key `Answer: ` in the following instance: <s> Question: [INST]Create a Recurrent Neural Network (RNN) Model in Python with Keras that takes in text data and generates predicted labels[/INST]\n",
            " Answer: ```python\n",
            "import keras\n",
            "from keras.preprocessing import sequence\n",
            "from keras.models import Sequential\n",
            "from keras.layers import Dense, Embedding, LSTM\n",
            "from keras.datasets import imdb\n",
            "\n",
            "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=20000)\n",
            "\n",
            "x_train = sequence.pad_sequences(x_train, maxlen=80)\n",
            "x_test = sequence.pad_sequences(x_test, maxlen=80)\n",
            "\n",
            "model = Sequential()\n",
            "model.add(Embedding(20000, 128))\n",
            "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
            "model.add(Dense(1, activation='sigmoid'))\n",
            "\n",
            "model.compile(loss='binary_crossentropy',\n",
            " optimizer='adam',\n",
            " metrics=['accuracy'])\n",
            "\n",
            "model.fit(x_train, y_train,\n",
            " batch_size=32,\n",
            " epochs=15,\n",
            " verbose=2,\n",
            " validation_data=(x_test, y_test))\n",
            "\n",
            "score, acc = model.evaluate(x_test, y_test,\n",
            " batch_size=32,\n",
            " verbose=2)\n",
            "```<unk><unk><unk><unk> This instance will be ignored in loss calculation. Note, if this happens often, consider increasing the `max_seq_length`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/trl/trainer/utils.py:187: UserWarning: Could not find response key `Answer: ` in the following instance: <s> Question: [INST]I'm a python developer, but when i'm putting my file inside a cgi-bin directory\n",
            "and running it through my browser its showing 500 internal server error.\n",
            "\n",
            "\n",
            "\n",
            "```\n",
            "#!/usr/bin/env python\n",
            "# -*- coding: UTF-8 -*-\n",
            "\n",
            "# enable debugging\n",
            "import cgitb\n",
            "cgitb.enable()\n",
            "\n",
            "print \"Content-Type: text/plain;charset=utf-8\"\n",
            "print\n",
            "\n",
            "print \"Hello World!\"\n",
            "\n",
            "```\n",
            "\n",
            "This is my test.py.\n",
            "Filelocation : [http://mywebsite.com/cgi-bin/test.py](http://mywebsite.com/cgi-bin/test.py)\n",
            "\n",
            "\n",
            ".htaccess file :\n",
            "\n",
            "\n",
            "\n",
            "```\n",
            " RewriteEngine on\n",
            " Options +ExecCGI AddHandler cgi-script cgi py\n",
            " Options +ExecCGI SetHandler cgi-script\n",
            "\n",
            "```\n",
            "\n",
            "Please give some suggestion!\n",
            "\n",
            "[/INST]\n",
            " Answer: It's possible that the error is caused by a file permission issue. Make sure that the CGI script has execute permissions (try chmod +x test.py). \n",
            "\n",
            "Another possibility is that the Python executable or other necessary libraries are not in the server's PATH. You can try specifying the path to the Python executable at the top of your script, like this: \n",
            "\n",
            "#!/path/to/python/executable\n",
            "\n",
            "If neither of these solutions work, you can try looking at the server's error logs for more information on the specific error that is occurring.<unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk> This instance will be ignored in loss calculation. Note, if this happens often, consider increasing the `max_seq_length`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/trl/trainer/utils.py:187: UserWarning: Could not find response key `Answer: ` in the following instance: <s> Question: [INST]Create a Python program that can search through two collections of data (collection 1 and collection 2) and calculate the probability that two items match one another[/INST]\n",
            " Answer: ```python\n",
            "# Import the necessary libraries\n",
            "import json \n",
            "from collections import OrderedDict \n",
            "import pandas as pd\n",
            "\n",
            "# Load the data \n",
            "with open('collection1.json') as json_file:\n",
            "    collection1 = json.load(json_file)\n",
            "\n",
            "with open('collection2.json') as json_file:\n",
            "    collection2 = json.load(json_file)\n",
            "\n",
            "# Create a DataFrame from both collections\n",
            "df1 = pd.DataFrame(collection1)\n",
            "df2 = pd.DataFrame(collection2)\n",
            "\n",
            "# Calculate the probability of matching items\n",
            "match_probs = {}\n",
            "\n",
            "for item1 in df1.item.unique():\n",
            "    for item2 in df2.item.unique():\n",
            "        item1_obs = df1.loc[df1['item'] == item1]\n",
            "        item2_obs = df2.loc[df2['item'] == item2]\n",
            "        match_prob = (len(item1_obs) * len(item2_obs)) / (len(df1) * len(df2))\n",
            "        match_probs[(item1,item2)] = match_prob\n",
            "\n",
            "# Print the results\n",
            "match_probs = OrderedDict(sorted(match_probs.items(), key=lambda x: x[1], reverse=True))\n",
            "print(match_probs)\n",
            "```<unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk> This instance will be ignored in loss calculation. Note, if this happens often, consider increasing the `max_seq_length`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/trl/trainer/utils.py:187: UserWarning: Could not find response key `Answer: ` in the following instance: <s> Question: [INST]I wanted to see if it was possible to define new keywords or, as they're called in [WAT's Destroy All Software talk](https://www.destroyallsoftware.com/talks/wat) when discussing Ruby, bare words, in Python.\n",
            "\n",
            "\n",
            "I came up with an answer that I couldn't find elsewhere, so I decided to share it Q&A style on StackOverflow.\n",
            "\n",
            "[/INST]\n",
            " Answer: Q: Is it possible to define new keywords in Python?\n",
            "\n",
            "A: No, it is not possible to define new keywords in Python. Keywords in Python are reserved for specific purposes and cannot be redefined or assigned new meanings. \n",
            "\n",
            "Q: Can we define bare words as keywords in Python?\n",
            "\n",
            "A: No, we cannot define bare words as keywords in Python. Unlike Ruby, Python does not allow for the use of bare words as keywords or syntax elements. All Python keywords must be explicitly defined and cannot be redefined or used for other purposes. \n",
            "\n",
            "Q: Is there a way to achieve similar functionality to define new keywords or bare words in Python?\n",
            "\n",
            "A: While we cannot define new keywords or bare words in Python, we can simulate their functionality using modules, classes, and functions. By creating custom modules, classes, or functions that perform specific tasks or have specific behaviors, we can emulate the functionality of keywords or bare words in our code. This approach is commonly used in Python to create domain-specific languages (DSLs) that allow developers to write code using familiar syntax and constructs that align with their specific needs.<unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk> This instance will be ignored in loss calculation. Note, if this happens often, consider increasing the `max_seq_length`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/trl/trainer/utils.py:187: UserWarning: Could not find response key `Answer: ` in the following instance: <s> Question: [INST]Implement a single-threaded web server using Python which can handle a maximum of 10 concurrent requests[/INST]\n",
            " Answer: ```python\n",
            "import socket \n",
            "  \n",
            "# socket object \n",
            "serversocket = socket.socket(socket.AF_INET, socket.SOCK_STREAM) \n",
            "  \n",
            "# ensures that multiple instances  \n",
            "# of the application cannot bind \n",
            "# to the same port \n",
            "serversocket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1) \n",
            "  \n",
            "# IP address and port of server \n",
            "IP_address = socket.gethostname()\n",
            "Port = 12345\n",
            "  \n",
            "# binds IP and port  \n",
            "serversocket.bind((IP_address, Port)) \n",
            "  \n",
            "# specifies number of connections  \n",
            "# the server can handle simultaneous \n",
            "serversocket.listen(10) \n",
            "  \n",
            "while True:\n",
            "  \n",
            "    # establishes connection \n",
            "    clientsocket, address = serversocket.accept()   \n",
            "   \n",
            "    # prints the address of the  \n",
            "    # client making the connection \n",
            "    print(\"Connection from \" + str(address))  \n",
            "  \n",
            "    # generates response \n",
            "    message = \"The server grants your request.\\r\\n\"\n",
            "  \n",
            "    # send an answer to the client    \n",
            "    clientsocket.send(message.encode('ascii')) \n",
            "  \n",
            "    # closes the connection \n",
            "    clientsocket.close()\n",
            "```<unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk> This instance will be ignored in loss calculation. Note, if this happens often, consider increasing the `max_seq_length`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/trl/trainer/utils.py:187: UserWarning: Could not find response key `Answer: ` in the following instance: <s> Question: [INST]Write a Python script to sort the given array using Heap sort algorithm [8, 5, 2, 1, 8][/INST]\n",
            " Answer: ```python\n",
            "def heapify(arr, n, i):\n",
            "    largest = i\n",
            "    l = 2 * i + 1  \n",
            "    r = 2 * i + 2\n",
            " \n",
            "    if l < n and arr[i] < arr[l]:\n",
            "        largest = l\n",
            " \n",
            "    if r < n and arr[largest] < arr[r]:\n",
            "        largest = r\n",
            " \n",
            "    if largest != i:\n",
            "        arr[i],arr[largest] = arr[largest],arr[i]\n",
            "        heapify(arr, n, largest)\n",
            " \n",
            "def heapSort(arr):\n",
            "    n = len(arr) \n",
            " \n",
            "    for i in range(n // 2 - 1, -1, -1):\n",
            "        heapify(arr, n, i)\n",
            " \n",
            "    for i in range(n-1, 0, -1):\n",
            "        arr[i], arr[0] = arr[0], arr[i]\n",
            "        heapify(arr, i, 0)\n",
            " \n",
            "arr = [8, 5, 2, 1, 8]\n",
            "heapSort(arr)\n",
            "n = len(arr) \n",
            "print (\"Sorted array is\") \n",
            "for i in range(n): \n",
            "    print (\"% d\" %arr[i]),\n",
            "```<unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk> This instance will be ignored in loss calculation. Note, if this happens often, consider increasing the `max_seq_length`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/trl/trainer/utils.py:187: UserWarning: Could not find response key `Answer: ` in the following instance: <s> Question: [INST]Generate a Python script to classify a dataset of features into two classes [height, weight, shoe_size, age, gender][/INST]\n",
            " Answer: ```python\n",
            "# Import necessary libraries\n",
            "import numpy as np\n",
            "from sklearn import tree\n",
            "\n",
            "# Create the two classes\n",
            "class_1 = [1, 0]\n",
            "class_2 = [0, 1]\n",
            "\n",
            "# Initialize the dataset of features\n",
            "data_set = np.array([ \n",
            "    [172, 80, 10, 28, 0],\n",
            "    [188, 78, 11, 33, 0],\n",
            "    [150, 55, 8, 21, 1],\n",
            "    [170, 68, 9, 22, 1],    \n",
            "])\n",
            "\n",
            "# Initialize the labels\n",
            "labels = np.array([class_1, class_2, class_2, class_1])\n",
            "\n",
            "# Create the decision tree classifier\n",
            "clf = tree.DecisionTreeClassifier()\n",
            "clf = clf.fit(data_set, labels)\n",
            "\n",
            "# Create a prediction\n",
            "prediction = clf.predict([[187,77,11,34,0]])\n",
            "\n",
            "# Output the prediction\n",
            "if prediction == class_1: \n",
            "    print(\"Prediction: Class 1\")\n",
            "elif prediction == class_2:\n",
            "    print(\"Prediction: Class 2\")\n",
            "```<unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk> This instance will be ignored in loss calculation. Note, if this happens often, consider increasing the `max_seq_length`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-98-3435b262f1ae>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    449\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trl_activate_neftune\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 451\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m         \u001b[0;31m# After training we make sure to retrieve back the original forward pass method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1930\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1931\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1932\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   1933\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1934\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2267\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccumulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2268\u001b[0;31m                     \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2270\u001b[0m                 if (\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3322\u001b[0m                 \u001b[0mscaled_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3323\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3324\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3326\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient_accumulation_steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2157\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlomo_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2158\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2159\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2161\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_trigger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    523\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m             )\n\u001b[0;32m--> 525\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    526\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    268\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 744\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    745\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mUHPET8RMGuL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JeBQbo8uJyMj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zlNRtVIwJyPP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}